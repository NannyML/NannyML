.. _chunk-data:
====
Chunking data
====

All the results generated by NannyML are calculated and presented on the level of data chunk. This guide will walk you
through the ways analyzed data can be split into chunks. If you feel like you don't know what *chunk* is, have a look
at :term:`Data Chunk`.

The examples below will be run based on performance estimation flow on NannyML synthetic dataset.
Set up first with:

.. code-block:: python

    >>> import nannyml as nml
    >>> idf_ref, df_ana, _ = nml.datasets.load_synthetic_sample()
    >>> imd = nml.extract_metadata(df_ref)
    >>> md.ground_truth_column_name = 'work_home_actual'



Time-based chunking
====
Time-based chunking is simply creating chunks based on time intervals. One chunk can contain all the observations
from single hour, day, week, month etc. In most cases such chunks will vary in length. Specify ``chunk_period`` argument
to get required split. See the
example with
quarters:

.. code-block:: python

    >>> cbpe = nml.CBPE(model_metadata=md, chunk_period="Q")
    >>> cbpe.fit(reference_data=df_ref)
    >>> est_perf = cbpe.estimate(df_ana)
    >>> est_perf.iloc[:3,:5]

+----+--------+---------------+-------------+---------------------+---------------------+
|    | key    |   start_index |   end_index | start_date          | end_date            |
+====+========+===============+=============+=====================+=====================+
|  0 | 2017Q3 |             0 |        1261 | 2017-08-31 00:00:00 | 2017-09-30 23:59:59 |
+----+--------+---------------+-------------+---------------------+---------------------+
|  1 | 2017Q4 |          1262 |        4951 | 2017-10-01 00:00:00 | 2017-12-31 23:59:59 |
+----+--------+---------------+-------------+---------------------+---------------------+
|  2 | 2018Q1 |          4952 |        8702 | 2018-01-01 00:00:00 | 2018-03-31 23:59:59 |
+----+--------+---------------+-------------+---------------------+---------------------+

.. note::
    Be aware that each calendar quarter will be taken into account, even if it is not full of records. Make sure to control
    for that while preparing the data. See example below.

    .. code-block:: python

        >>> est_perf.iloc[-2:,:5]

    +----+--------+---------------+-------------+---------------------+---------------------+
    |    | key    |   start_index |   end_index | start_date          | end_date            |
    +====+========+===============+=============+=====================+=====================+
    | 13 | 2020Q4 |         46219 |       49989 | 2020-10-01 00:00:00 | 2020-12-31 23:59:59 |
    +----+--------+---------------+-------------+---------------------+---------------------+
    | 14 | 2021Q1 |         49990 |       49999 | 2021-01-01 00:00:00 | 2021-01-01 23:59:59 |
    +----+--------+---------------+-------------+---------------------+---------------------+

Possible time offsets are listed in the table below:

+------------+------------+
| Header 1   | Header 2   |
+============+============+
| S          | second     |
+------------+------------+
| T, min     | minute     |
+------------+------------+
| H          | hour       |
+------------+------------+
| D          | day        |
+------------+------------+
| W          | week       |
+------------+------------+
| M          | month      |
+------------+------------+
| Q          | quarter    |
+------------+------------+
| A, y       | year       |
+------------+------------+


Size-based chunking
====
Chunks can be of fixed size i.e. each chunk contains the same number of observations. Set this up by specifying
``chunk_size`` parameter:

.. code-block:: python

    >>> cbpe = nml.CBPE(model_metadata=md, chunk_size=3500)
    >>> cbpe.fit(reference_data=df_ref)
    >>> est_perf = cbpe.estimate(df_ana)
    >>> est_perf.iloc[:3,:5]

+----+--------------+---------------+-------------+---------------------+---------------------+
|    | key          |   start_index |   end_index | start_date          | end_date            |
+====+==============+===============+=============+=====================+=====================+
|  0 | [0:3499]     |             0 |        3499 | 2017-08-31 00:00:00 | 2017-11-26 23:59:59 |
+----+--------------+---------------+-------------+---------------------+---------------------+
|  1 | [3500:6999]  |          3500 |        6999 | 2017-11-26 00:00:00 | 2018-02-18 23:59:59 |
+----+--------------+---------------+-------------+---------------------+---------------------+
|  2 | [7000:10499] |          7000 |       10499 | 2018-02-18 00:00:00 | 2018-05-14 23:59:59 |
+----+--------------+---------------+-------------+---------------------+---------------------+


.. note::
    If the number observations is not divisible by the chunk size required, the number of observation equal to the
    reminder of a division will be dropped. This ensures that each chunk has the same size, but in worst case
    scenario it results in dropping ``chunk_size-1`` rows. See:

    .. code-block:: python

        >>> est_perf.iloc[-2:,:5]

    +----+---------------+---------------+-------------+---------------------+---------------------+
    |    | key           |   start_index |   end_index | start_date          | end_date            |
    +====+===============+===============+=============+=====================+=====================+
    | 12 | [42000:45499] |         42000 |       45499 | 2020-06-18 00:00:00 | 2020-09-13 23:59:59 |
    +----+---------------+---------------+-------------+---------------------+---------------------+
    | 13 | [45500:48999] |         45500 |       48999 | 2020-09-13 00:00:00 | 2020-12-08 23:59:59 |
    +----+---------------+---------------+-------------+---------------------+---------------------+

    .. code-block:: python

        >>> est_perf.index.max
        5000


Number-based chunking
====
The total number of chunks can be fixed by ``chunk_number`` parameter:

.. code-block:: python

    >>> cbpe = nml.CBPE(model_metadata=md, chunk_number=9)
    >>> cbpe.fit(reference_data=df_ref)
    >>> est_perf = cbpe.estimate(df_ana)
    >>> len(est_perf)
    >>> 9

.. note::
    Created chunks will be equal in size. If number of observations is not divisible by ``chunk_number`` then the
    number of observations equal to the residual of the division will be dropped. See:

    .. code-block:: python

        >>>> est_perf.iloc[-2:,:5]

    +----+---------------+---------------+-------------+---------------------+---------------------+
    |    | key           |   start_index |   end_index | start_date          | end_date            |
    +====+===============+===============+=============+=====================+=====================+
    |  7 | [38885:44439] |         38885 |       44439 | 2020-04-03 00:00:00 | 2020-08-18 23:59:59 |
    +----+---------------+---------------+-------------+---------------------+---------------------+
    |  8 | [44440:49994] |         44440 |       49994 | 2020-08-18 00:00:00 | 2021-01-01 23:59:59 |
    +----+---------------+---------------+-------------+---------------------+---------------------+

.. note::
    The same splitting rule is always applied to the dataset used to fitting (``reference``) and the dataset of
    interest (in the presented case - ``analysis``). Unless these two data sets are of the same size, the chunk sizes
    will be different. Additionally, if the data drift or performance estimation is calculated on concatenated
    ``reference`` and ``analysis`` the results presented for ``reference`` will be calculated on different chunks
    than they were fitted.

Different partitions within one chunk
====
If you want to get performance estimation or data drift results for a dataset that contains two
partitions - *reference* and *analysis* (see :term:`Partition`), most likely
there will be a chunk that contains both of them. We call it transition chunk. All the chunks before belong to
*reference* period
and all after, based on *analysis* period, are *actual* results. This is especially important for Performance Estimation
(# TODO naming?), where *reference* period should be treated like you treat your train set when modelling whereas
*analysis* is like test - the quality of estimation on the *reference* will most likely be much better than on
*analysis*.

It may happen that there is no transition chunk, in that case (# TODO)
