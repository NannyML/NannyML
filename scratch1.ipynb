{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nannyml as nml\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/carterblair/NannyMLdev/nannyml/nannyml']\n",
      "0.8.2\n"
     ]
    }
   ],
   "source": [
    "# print the path of nml\n",
    "print(nml.__path__)\n",
    "print(nml.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _estimate_chunk(chunk: int):\n",
    "    # make an empty dictionary\n",
    "    dict = {}\n",
    "    for i in range(4):\n",
    "        # make a dictionary with the chunk number and the value from multiplying it bny i\n",
    "        dict[f\"{i}\"] = chunk * i\n",
    "    print(dict)\n",
    "    print()\n",
    "    return dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 0, '1': 1, '2': 2, '3': 3}\n",
      "\n",
      "{'0': 0, '1': 2, '2': 4, '3': 6}\n",
      "\n",
      "{'0': 0, '1': 3, '2': 6, '3': 9}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chunks = [1, 2, 3]\n",
    "\n",
    "res = pd.DataFrame.from_records(\n",
    "    [\n",
    "        {\n",
    "            'key': 'test',\n",
    "            **_estimate_chunk(chunk),\n",
    "        }\n",
    "        for chunk in chunks\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    key  0  1  2  3\n",
       "0  test  0  1  2  3\n",
       "1  test  0  2  4  6\n",
       "2  test  0  3  6  9"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 = {'0': 0, '1': 1, '2': 2, '3': 3}\n",
    "dict2 = {'5': 5, '6': 6, '7': 7, '8': 8}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 0, '1': 1, '2': 2, '3': 3, '5': 5, '6': 6, '7': 7, '8': 8}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict1.update(dict2)\n",
    "dict1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ****************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num tp:  3\n",
      "num tn:  3\n",
      "num fp:  1\n",
      "num fn:  2\n"
     ]
    }
   ],
   "source": [
    "y_true = [0, 1, 1, 0, 1, 1, 0, 1, 0]\n",
    "y_pred = [0, 1, 0, 0, 1, 1, 1, 0, 0]\n",
    "\n",
    "# find number of true positives\n",
    "print(\"num tp: \", np.sum(np.logical_and(y_true, y_pred)))\n",
    "print(\"num tn: \", np.sum(np.logical_and(np.logical_not(y_true), np.logical_not(y_pred))))\n",
    "print(\"num fp: \", np.sum(np.logical_and(np.logical_not(y_true), y_pred)))\n",
    "print(\"num fn: \", np.sum(np.logical_and(y_true, np.logical_not(y_pred))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_df, ana_df, _ = nml.load_synthetic_binary_classification_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BinaryClassificationAUROC.__init__() got an unexpected keyword argument 'normalize_confusion_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cbpe \u001b[39m=\u001b[39m nml\u001b[39m.\u001b[39;49mCBPE(\n\u001b[0;32m      2\u001b[0m         y_pred_proba\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39my_pred_proba\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m      3\u001b[0m         y_pred\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39my_pred\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m      4\u001b[0m         y_true\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mwork_home_actual\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m      5\u001b[0m         problem_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mclassification_binary\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m      6\u001b[0m         metrics\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mroc_auc\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mf1\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mprecision\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrecall\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mspecificity\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39maccuracy\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtrue_positive\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtrue_negative\u001b[39;49m\u001b[39m'\u001b[39;49m,\\\n\u001b[0;32m      7\u001b[0m                  \u001b[39m'\u001b[39;49m\u001b[39mfalse_positive\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mfalse_negative\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m      8\u001b[0m         normalize_confusion_matrix \u001b[39m=\u001b[39;49m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m      9\u001b[0m         chunk_size\u001b[39m=\u001b[39;49m\u001b[39m20_000\u001b[39;49m)\u001b[39m.\u001b[39mfit(ref_df)\n\u001b[0;32m     10\u001b[0m result \u001b[39m=\u001b[39m cbpe\u001b[39m.\u001b[39mestimate(ana_df)\n\u001b[0;32m     11\u001b[0m df \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mto_df()\n",
      "File \u001b[1;32mc:\\Users\\TheBl\\OneDrive\\Desktop\\AA_Carter\\nannyml\\nannyml\\performance_estimation\\confidence_based\\cbpe.py:151\u001b[0m, in \u001b[0;36mCBPE.__init__\u001b[1;34m(self, metrics, y_pred, y_pred_proba, y_true, problem_type, timestamp_column_name, chunk_size, chunk_number, chunk_period, chunker, calibration, calibrator, normalize_confusion_matrix)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(metrics, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    150\u001b[0m     metrics \u001b[39m=\u001b[39m [metrics]\n\u001b[1;32m--> 151\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetrics \u001b[39m=\u001b[39m [\n\u001b[0;32m    152\u001b[0m     MetricFactory\u001b[39m.\u001b[39mcreate(\n\u001b[0;32m    153\u001b[0m         metric,\n\u001b[0;32m    154\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproblem_type,\n\u001b[0;32m    155\u001b[0m         y_pred_proba\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_pred_proba,\n\u001b[0;32m    156\u001b[0m         y_pred\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_pred,\n\u001b[0;32m    157\u001b[0m         y_true\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_true,\n\u001b[0;32m    158\u001b[0m         timestamp_column_name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimestamp_column_name,\n\u001b[0;32m    159\u001b[0m         chunker\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunker,\n\u001b[0;32m    160\u001b[0m         normalize_confusion_matrix\u001b[39m=\u001b[39mnormalize_confusion_matrix,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    162\u001b[0m     \u001b[39mfor\u001b[39;00m metric \u001b[39min\u001b[39;00m metrics\n\u001b[0;32m    163\u001b[0m ]\n\u001b[0;32m    165\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfidence_upper_bound \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    166\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfidence_lower_bound \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\TheBl\\OneDrive\\Desktop\\AA_Carter\\nannyml\\nannyml\\performance_estimation\\confidence_based\\cbpe.py:152\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(metrics, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    150\u001b[0m     metrics \u001b[39m=\u001b[39m [metrics]\n\u001b[0;32m    151\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetrics \u001b[39m=\u001b[39m [\n\u001b[1;32m--> 152\u001b[0m     MetricFactory\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m    153\u001b[0m         metric,\n\u001b[0;32m    154\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproblem_type,\n\u001b[0;32m    155\u001b[0m         y_pred_proba\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49my_pred_proba,\n\u001b[0;32m    156\u001b[0m         y_pred\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49my_pred,\n\u001b[0;32m    157\u001b[0m         y_true\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49my_true,\n\u001b[0;32m    158\u001b[0m         timestamp_column_name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimestamp_column_name,\n\u001b[0;32m    159\u001b[0m         chunker\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunker,\n\u001b[0;32m    160\u001b[0m         normalize_confusion_matrix\u001b[39m=\u001b[39;49mnormalize_confusion_matrix,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    162\u001b[0m     \u001b[39mfor\u001b[39;00m metric \u001b[39min\u001b[39;00m metrics\n\u001b[0;32m    163\u001b[0m ]\n\u001b[0;32m    165\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfidence_upper_bound \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    166\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfidence_lower_bound \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\TheBl\\OneDrive\\Desktop\\AA_Carter\\nannyml\\nannyml\\performance_estimation\\confidence_based\\metrics.py:239\u001b[0m, in \u001b[0;36mMetricFactory.create\u001b[1;34m(cls, key, use_case, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    234\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmetric \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is currently not supported for use case \u001b[39m\u001b[39m{\u001b[39;00muse_case\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    235\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease specify another metric or use one of these supported model types for this metric: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    236\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m[md\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mmd\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mregistry[key]]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m     )\n\u001b[0;32m    238\u001b[0m metric_class \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mregistry[key][use_case]\n\u001b[1;32m--> 239\u001b[0m \u001b[39mreturn\u001b[39;00m metric_class(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: BinaryClassificationAUROC.__init__() got an unexpected keyword argument 'normalize_confusion_matrix'"
     ]
    }
   ],
   "source": [
    "cbpe = nml.CBPE(\n",
    "    y_pred_proba='y_pred_proba',\n",
    "    y_pred='y_pred',\n",
    "    y_true='work_home_actual',\n",
    "    problem_type='classification_binary',\n",
    "    metrics=[\n",
    "        'roc_auc',\n",
    "        'f1',\n",
    "        'precision',\n",
    "        'recall',\n",
    "        'specificity',\n",
    "        'accuracy',\n",
    "        'true_positive',\n",
    "        'true_negative',\n",
    "        'false_positive',\n",
    "        'false_negative',\n",
    "    ],\n",
    "    normalize_confusion_matrix=None,\n",
    "    chunk_size=20_000,\n",
    ").fit(ref_df)\n",
    "result = cbpe.estimate(ana_df)\n",
    "df = result.to_df()\n",
    "\n",
    "sut = result.filter(period='analysis').to_df()[[('chunk', 'key')] + [(m.column_name, 'value') for m in result.metrics]]\n",
    "\n",
    "\n",
    "print(\"(\")\n",
    "print(\"\\t\\t\\t{'chunk_size\\':20000, \\'cm_normalization\\' = None},\")\n",
    "print(\"\\t\\t\\tpd.DataFrame(\")\n",
    "print(\"\\t\\t\\t\\t{\")\n",
    "# print the chunk keys in this format: 'key': ['[0:19999]', '[20000:49999]']\n",
    "print(\"\\t\\t\\t\\t\\t\\'key\\'\", \":\", sut['chunk']['key'].unique().tolist(), \",\")\n",
    "\n",
    "# print the column names and the list of values in this format: 'roc_auc': [0.5, 0.5] using f strings\n",
    "for col in sut.columns:\n",
    "    if col[0] != 'chunk':\n",
    "        print(f'\\t\\t\\t\\t\\t\\'estimated_{col[0]}\\': {sut[col].values.tolist()},')\n",
    "\n",
    "print(\"\\t\\t\\t\\t}\")\n",
    "print(\"\\t\\t\\t),\")\n",
    "print(\"\\t\\t),\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7015d45318b7e47f58bc453f811cb2d0d7fdbd19d5bcf220fdd25c0dc451c826"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
